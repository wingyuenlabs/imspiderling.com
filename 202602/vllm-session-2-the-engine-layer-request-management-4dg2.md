---
Title: vLLM â€” Session 2: The Engine Layer â€” Request Management
Description: 
Author: Ben
Date: 2026-02-01T22:00:10.000Z
Robots: noindex,nofollow
Template: index
---
<p><em>This is part of my vLLM learning series. In this session, I cover Step 2 (The Engine Layer).</em></p>

<blockquote>
<p><strong>Note</strong>: This content was generated by Claude, grounded on the actual<br>
<a href="https://github.com/vllm-project/vllm" rel="noopener noreferrer">vLLM</a> codebase. It is intended for personal<br>
learning only and may contain inaccuracies. Always verify against the<br>
original source code and official documentation.</p>
</blockquote>

<p><strong>Topic</strong>: vLLM<br>
<strong>Date</strong>: 2026-02-01<br>
<strong>Sections covered</strong>: Step 2 (The Engine Layer)<br>
<strong>Prerequisites</strong>: Session 1 â€” LLM class, SamplingParams, generate() flow, RequestOutput</p>


<h2>
  
  
  Review
</h2>

<p>In Session 1, we learned that the <code>LLM</code> class is a thin wrapper around <code>LLMEngine</code>. When you call <code>llm.generate()</code>, the flow is:</p>

<ol>
<li>
<code>_validate_and_add_requests()</code> â€” pairs prompts with <code>SamplingParams</code>
</li>
<li>
<code>_run_engine()</code> â€” loops <code>self.llm_engine.step()</code> until all requests finish</li>
<li>Returns sorted <code>RequestOutput</code> objects</li>
</ol>

<p>We saw that <code>LLM.__init__()</code> calls <code>LLMEngine.from_engine_args()</code> â€” but we treated the engine as a black box. Today we open that box.</p>

<p>The key question: <strong>What happens inside <code>llm_engine.step()</code>?</strong> The answer involves three components: <code>InputProcessor</code>, <code>EngineCoreClient</code>, and <code>OutputProcessor</code>.</p>


<h2>
  
  
  Today's Material
</h2>
<h3>
  
  
  1. LLMEngine â€” The Orchestrator
</h3>

<p>The <code>LLMEngine</code> sits between the user-facing <code>LLM</code> class and the core scheduling/execution machinery. Its job is to:</p>

<ol>
<li>
<strong>Preprocess</strong> inputs (tokenize prompts, handle multimodal data)</li>
<li>
<strong>Relay</strong> preprocessed requests to the engine core</li>
<li>
<strong>Postprocess</strong> raw outputs (detokenize, format for the user)
</li>
</ol>
<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/v1/engine/llm_engine.py
</span><span class="k">class</span> <span class="nc">LLMEngine</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vllm_config</span><span class="p">,</span> <span class="n">executor_class</span><span class="p">,</span> <span class="n">log_stats</span><span class="p">,</span> <span class="p">...):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">engine_core</span> <span class="o">=</span> <span class="nc">EngineCoreClient</span><span class="p">(...)</span>    <span class="c1"># Talks to core
</span>        <span class="n">self</span><span class="p">.</span><span class="n">input_processor</span> <span class="o">=</span> <span class="nc">InputProcessor</span><span class="p">(...)</span>   <span class="c1"># Tokenize inputs
</span>        <span class="n">self</span><span class="p">.</span><span class="n">output_processor</span> <span class="o">=</span> <span class="nc">OutputProcessor</span><span class="p">(...)</span> <span class="c1"># Format outputs
</span>
    <span class="k">def</span> <span class="nf">add_request</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">request_id</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="p">...):</span>
        <span class="sh">"""</span><span class="s">Preprocess and send request to engine core.</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">One iteration: get outputs from core, process, return.</span><span class="sh">"""</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_engine_args</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">engine_args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="sh">"</span><span class="s">LLMEngine</span><span class="sh">"</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Factory: parse args -&gt; VllmConfig -&gt; create engine.</span><span class="sh">"""</span>
</code></pre>

</div>


<p>Think of <code>LLMEngine</code> as a <strong>translator</strong>: it speaks "user language" (strings, Python objects) on one side and "engine language" (token IDs, <code>msgspec</code> structs) on the other.</p>

<blockquote>
<p><strong>ğŸ“ Note:</strong><br>
vLLM has undergone a major architectural evolution. The <code>v1/</code> directory contains the current architecture. Older code in the root <code>vllm/engine/</code> directory is the legacy (v0) engine. When reading code, focus on <code>vllm/v1/</code> â€” that's where active development happens.</p>
</blockquote>
<h3>
  
  
  2. The Factory: from_engine_args()
</h3>

<p>Before exploring the runtime flow, let's see how <code>LLMEngine</code> gets created:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/v1/engine/llm_engine.py
</span><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">from_engine_args</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">engine_args</span><span class="p">,</span> <span class="n">usage_context</span><span class="p">,</span> <span class="p">...)</span> <span class="o">-&gt;</span> <span class="sh">"</span><span class="s">LLMEngine</span><span class="sh">"</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">Factory: parse args -&gt; VllmConfig -&gt; create engine.</span><span class="sh">"""</span>
    <span class="n">vllm_config</span> <span class="o">=</span> <span class="n">engine_args</span><span class="p">.</span><span class="nf">create_engine_config</span><span class="p">()</span>
    <span class="c1"># vllm_config is a VllmConfig that bundles:
</span>    <span class="c1">#   ModelConfig, CacheConfig, ParallelConfig,
</span>    <span class="c1">#   SchedulerConfig, DeviceConfig, LoadConfig, ...
</span>
    <span class="n">executor_class</span> <span class="o">=</span> <span class="n">Executor</span><span class="p">.</span><span class="nf">get_class</span><span class="p">(</span><span class="n">vllm_config</span><span class="p">)</span>
    <span class="c1"># Selects UniProcExecutor, MultiprocExecutor, or RayDistributedExecutor
</span>
    <span class="k">return</span> <span class="nf">cls</span><span class="p">(</span><span class="n">vllm_config</span><span class="o">=</span><span class="n">vllm_config</span><span class="p">,</span>
               <span class="n">executor_class</span><span class="o">=</span><span class="n">executor_class</span><span class="p">,</span>
               <span class="n">usage_context</span><span class="o">=</span><span class="n">usage_context</span><span class="p">,</span> <span class="p">...)</span>
</code></pre>

</div>



<p>This is a classic factory pattern. The user provides simple arguments (<code>model="meta-llama/..."</code>, <code>tensor_parallel_size=2</code>), and the factory:</p>

<ol>
<li>Parses them into a structured <code>VllmConfig</code>
</li>
<li>Selects the right executor class based on configuration</li>
<li>Constructs the engine with all dependencies wired up</li>
</ol>

<p><strong>Why this matters</strong>: The factory is where all of vLLM's auto-configuration happens. It determines dtype (auto-selects fp16/bf16 based on GPU capability), figures out how many blocks fit in memory, and selects the appropriate attention backend.</p>

<h3>
  
  
  3. InputProcessor â€” From Strings to Tokens
</h3>

<p>When <code>add_request()</code> is called, the first thing that happens is input processing:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/v1/engine/input_processor.py
</span><span class="k">class</span> <span class="nc">InputProcessor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vllm_config</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="p">...):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mm_processor</span> <span class="o">=</span> <span class="p">...</span>  <span class="c1"># Multimodal input processor
</span>
    <span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">request_id</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="p">...)</span> <span class="o">-&gt;</span> <span class="n">EngineCoreRequest</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Tokenize prompt, process multimodal inputs,
           create EngineCoreRequest.</span><span class="sh">"""</span>
</code></pre>

</div>



<p>The processor handles several input formats:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># Users can provide prompts in multiple ways:
</span><span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello world</span><span class="sh">"</span><span class="p">)</span>                           <span class="c1"># Plain string
</span><span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">({</span><span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Hello world</span><span class="sh">"</span><span class="p">})</span>               <span class="c1"># Dict with string
</span><span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">({</span><span class="sh">"</span><span class="s">prompt_token_ids</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">15496</span><span class="p">,</span> <span class="mi">995</span><span class="p">]})</span>      <span class="c1"># Pre-tokenized
</span><span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">({</span>                                         <span class="c1"># Multimodal
</span>    <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What</span><span class="sh">'</span><span class="s">s in this image?</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">multi_modal_data</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">image</span><span class="sh">"</span><span class="p">:</span> <span class="n">image_data</span><span class="p">}</span>
<span class="p">})</span>
</code></pre>

</div>



<p>No matter what format you use, <code>InputProcessor.process()</code> normalizes it into an <code>EngineCoreRequest</code> â€” the standard wire format for the engine core.</p>

<p><strong>The tokenization step</strong> converts your string prompt into token IDs:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>"What is the capital of France?"
    â†’ tokenizer.encode()
    â†’ [1, 1724, 338, 278, 7483, 310, 3444, 29973]
</code></pre>

</div>



<blockquote>
<p><strong>ğŸ’¡ Tip:</strong><br>
If you already have token IDs (e.g., from your own tokenizer or preprocessing pipeline), pass <code>{"prompt_token_ids": [...]}</code> to skip redundant tokenization. This saves CPU time for high-throughput applications.</p>
</blockquote>

<h3>
  
  
  4. EngineCoreRequest â€” The Wire Format
</h3>

<p>The output of <code>InputProcessor</code> is an <code>EngineCoreRequest</code>:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/v1/engine/__init__.py
</span><span class="k">class</span> <span class="nc">EngineCoreRequest</span><span class="p">(</span><span class="n">msgspec</span><span class="p">.</span><span class="n">Struct</span><span class="p">):</span>
    <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">mm_features</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">MultiModalFeatureSpec</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">pooling_params</span><span class="p">:</span> <span class="n">PoolingParams</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">eos_token_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">arrival_time</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">lora_request</span><span class="p">:</span> <span class="n">LoRARequest</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">cache_salt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">data_parallel_rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">prompt_embeds</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">client_index</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">current_wave</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">priority</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">trace_headers</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">resumable</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">external_req_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="bp">None</span>
</code></pre>

</div>



<p>Why is this a separate type from <code>Request</code> (which the scheduler uses internally)?</p>

<p><strong>Separation of concerns</strong>:</p>

<ul>
<li>
<code>EngineCoreRequest</code> is the <strong>transport</strong> format â€” designed for serialization across process boundaries</li>
<li>
<code>Request</code> (in the scheduler) is the <strong>runtime</strong> format â€” tracks mutable state like <code>num_computed_tokens</code>, allocated blocks, output tokens</li>
</ul>

<p>This separation is important because vLLM can run in <strong>multiprocess mode</strong>: the FastAPI server and <code>InputProcessor</code> run in one process, while the <code>EngineCore</code> (scheduler + executor) runs in another. The <code>EngineCoreRequest</code> gets serialized with <code>msgspec.msgpack.encode()</code>, sent over a ZMQ socket, and deserialized on the other side.<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>Process 1 (Frontend)           Process 2 (Engine Core)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  InputProcessor  â”‚           â”‚    Scheduler     â”‚
â”‚       â†“          â”‚           â”‚       â†“          â”‚
â”‚ EngineCoreRequestâ”‚â”€â”€ZMQâ”€â”€â†’   â”‚    Request       â”‚
â”‚                  â”‚           â”‚  (mutable state)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>

</div>



<blockquote>
<p><strong>ğŸ“ Note:</strong><br>
<strong>Why msgspec instead of pickle or JSON?</strong> <code>msgspec.msgpack</code> is 10-50x faster than pickle for structured data and produces smaller payloads than JSON. For a system processing thousands of requests per second, serialization overhead directly impacts throughput. This is not premature optimization â€” it's a measured bottleneck.</p>
</blockquote>

<h3>
  
  
  5. EngineCoreClient â€” Bridging Processes
</h3>

<p><code>EngineCoreClient</code> abstracts the communication between the engine layer and the engine core:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># Conceptual interface:
</span><span class="k">class</span> <span class="nc">EngineCoreClient</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">add_request</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">EngineCoreRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Send request to the core.</span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">get_output</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">EngineCoreOutput</span><span class="p">]:</span>
        <span class="sh">"""</span><span class="s">Get completed/streaming outputs from the core.</span><span class="sh">"""</span>
</code></pre>

</div>



<p>The client has two modes:</p>

<div class="table-wrapper-paragraph"><table>
<thead>
<tr>
<th>Mode</th>
<th>When</th>
<th>How it works</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>In-process</strong></td>
<td>
<code>LLM</code> class (offline)</td>
<td>Direct function calls to <code>EngineCore</code>
</td>
</tr>
<tr>
<td><strong>Multiprocess</strong></td>
<td>API server</td>
<td>ZMQ sockets between processes</td>
</tr>
</tbody>
</table></div>

<p>In <strong>in-process mode</strong> (what you get with the <code>LLM</code> class), <code>EngineCoreClient</code> directly calls methods on an <code>EngineCore</code> object in the same process. No serialization overhead.</p>

<p>In <strong>multiprocess mode</strong> (the OpenAI-compatible server), <code>EngineCoreClient</code> serializes requests with <code>msgspec.msgpack</code>, sends them over ZMQ, and the <code>EngineCore</code> process deserializes and processes them. This keeps the FastAPI event loop responsive while heavy inference runs in a separate process.<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># Simplified view of multiprocess communication:
# Frontend process:
</span><span class="n">encoded</span> <span class="o">=</span> <span class="n">msgspec</span><span class="p">.</span><span class="n">msgpack</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">engine_core_request</span><span class="p">)</span>
<span class="n">zmq_socket</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>

<span class="c1"># Engine core process:
</span><span class="n">data</span> <span class="o">=</span> <span class="n">zmq_socket</span><span class="p">.</span><span class="nf">recv</span><span class="p">()</span>
<span class="n">request</span> <span class="o">=</span> <span class="n">msgspec</span><span class="p">.</span><span class="n">msgpack</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">EngineCoreRequest</span><span class="p">)</span>
<span class="n">scheduler</span><span class="p">.</span><span class="nf">add_request</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>
</code></pre>

</div>



<p><strong>Why this matters</strong>: This two-process architecture is critical for production deployments. Without it, long-running model forward passes on the GPU would block the HTTP server from accepting new requests.</p>

<h3>
  
  
  6. The step() Method â€” One Iteration
</h3>

<p>Now we can understand what happens in each call to <code>llm_engine.step()</code>:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/v1/engine/llm_engine.py (simplified)
</span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">]:</span>
    <span class="c1"># 1. Get raw outputs from the engine core
</span>    <span class="n">engine_core_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">engine_core</span><span class="p">.</span><span class="nf">get_output</span><span class="p">()</span>

    <span class="c1"># 2. Process outputs: detokenize, format, check completion
</span>    <span class="n">request_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">output_processor</span><span class="p">.</span><span class="nf">process_outputs</span><span class="p">(</span>
        <span class="n">engine_core_outputs</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">request_outputs</span>
</code></pre>

</div>



<p>Each <code>step()</code> returns a list of <code>RequestOutput</code> objects â€” some may be streaming (partial), others may be finished. The <code>_run_engine()</code> loop in <code>LLM</code> collects the finished ones.</p>

<p>But what triggers the core to actually run inference? In in-process mode, <code>get_output()</code> internally calls <code>engine_core.step()</code> which runs the scheduler + model execution. In multiprocess mode, the engine core runs its own loop continuously, and <code>get_output()</code> just reads from a queue.</p>

<h3>
  
  
  7. OutputProcessor â€” From Tokens to Text
</h3>

<p>The <code>OutputProcessor</code> is the mirror of <code>InputProcessor</code>:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/v1/engine/output_processor.py
</span><span class="k">class</span> <span class="nc">OutputProcessor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">log_stats</span><span class="p">,</span> <span class="p">...):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_states</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">RequestState</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
</code></pre>

</div>



<p>It receives <code>EngineCoreOutput</code> (raw token IDs from the core) and produces <code>RequestOutput</code> (user-facing results). The key operations:</p>

<ol>
<li>
<strong>Accumulate tokens</strong> â€” Maintains a running state per request</li>
<li>
<strong>Detokenize</strong> â€” Converts token IDs back to text using the tokenizer</li>
<li>
<strong>Handle streaming modes</strong> â€” <code>CUMULATIVE</code> returns the full text so far; <code>DELTA</code> returns only new tokens</li>
<li>
<strong>Track completion</strong> â€” Checks <code>finish_reason</code> to know when a request is done
</li>
</ol>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># EngineCoreOutput â€” what the core produces:
</span><span class="k">class</span> <span class="nc">EngineCoreOutput</span><span class="p">(</span><span class="n">msgspec</span><span class="p">.</span><span class="n">Struct</span><span class="p">):</span>
    <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">new_token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>          <span class="c1"># Newly generated tokens this step
</span>    <span class="n">new_logprobs</span><span class="p">:</span> <span class="n">LogprobsLists</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">finish_reason</span><span class="p">:</span> <span class="n">FinishReason</span> <span class="o">|</span> <span class="bp">None</span>  <span class="c1"># STOP, LENGTH, ABORT, or None
</span>    <span class="n">stop_reason</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="bp">None</span>
    <span class="n">num_cached_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre>

</div>



<p>The transformation:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>EngineCoreOutput                          RequestOutput
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ request_id: "req-42" â”‚                 â”‚ request_id: "req-42"    â”‚
â”‚ new_token_ids: [464] â”‚   detokenize    â”‚ prompt: "Hello"         â”‚
â”‚ finish_reason: None  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’    â”‚ outputs: [              â”‚
â”‚                      â”‚                 â”‚   CompletionOutput(     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚     text: " world",     â”‚
                                         â”‚     token_ids: [464],   â”‚
                                         â”‚     finish_reason: None â”‚
                                         â”‚   )                     â”‚
                                         â”‚ ]                       â”‚
                                         â”‚ finished: False         â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>

</div>



<blockquote>
<p><strong>âš ï¸ Warning:</strong><br>
<strong>Detokenization is not trivially reversible.</strong> Many tokenizers use byte-level BPE, where a single token might represent part of a multi-byte UTF-8 character. The <code>OutputProcessor</code> handles these edge cases â€” if a token produces an incomplete character, it buffers bytes until a valid character is formed. This is why you sometimes see "garbled" output when accessing raw <code>token_ids</code> without proper detokenization.</p>
</blockquote>

<h3>
  
  
  8. Putting It All Together â€” The Full Request Lifecycle
</h3>

<p>Let's trace a request from start to finish:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>User calls: llm.generate(["What is AI?"], SamplingParams(max_tokens=20))

1. LLM.generate()
   â””â†’ _validate_and_add_requests()
      â””â†’ llm_engine.add_request(request_id="0", prompt="What is AI?", params=...)
         â””â†’ InputProcessor.process()
            - Tokenize: "What is AI?" â†’ [1, 1724, 338, 319, 29902, 29973]
            - Create EngineCoreRequest(request_id="0",
                                       prompt_token_ids=[1, 1724, ...],
                                       sampling_params=...,
                                       arrival_time=time.time())
         â””â†’ engine_core.add_request(engine_core_request)

2. LLM._run_engine()
   while has_unfinished_requests():
     â””â†’ llm_engine.step()
        â””â†’ engine_core.get_output()
           - Core runs: schedule â†’ execute model â†’ sample tokens
           - Returns EngineCoreOutput(request_id="0",
                                      new_token_ids=[23435],
                                      finish_reason=None)
        â””â†’ OutputProcessor.process_outputs()
           - Detokenize [23435] â†’ " Artificial"
           - Accumulate: text = " Artificial"
           - Return RequestOutput(finished=False, ...)

     ... more steps, generating tokens one at a time ...

     â””â†’ llm_engine.step()  (final iteration)
        â””â†’ engine_core.get_output()
           - Returns EngineCoreOutput(request_id="0",
                                      new_token_ids=[29889],
                                      finish_reason=FinishReason.LENGTH)
        â””â†’ OutputProcessor.process_outputs()
           - Detokenize [29889] â†’ "."
           - Accumulate: text = " Artificial intelligence is..."
           - finish_reason = "length" (hit max_tokens=20)
           - Return RequestOutput(finished=True, ...)

3. _run_engine() collects finished output, sorts by request_id, returns
</code></pre>

</div>



<blockquote>
<p><strong>ğŸ“ Note:</strong><br>
In practice, the engine core doesn't generate just one token per step. With continuous batching, a single <code>step()</code> processes tokens for <strong>all active requests simultaneously</strong>. If there are 50 active requests, one GPU forward pass generates the next token for all 50. The <code>OutputProcessor</code> then demultiplexes the results back to individual <code>RequestOutput</code> objects.</p>
</blockquote>




<h2>
  
  
  Exercises
</h2>

<h3>
  
  
  Exercise 1: Component Identification
</h3>

<p><strong>Difficulty</strong>: Beginner<br>
<strong>Goal</strong>: Verify you can identify the role of each engine layer component</p>

<p>For each of the following operations, name which component handles it:</p>

<ol>
<li>Converting the string <code>"Hello world"</code> into token IDs <code>[15496, 995]</code>
</li>
<li>Deciding which requests get GPU time this iteration</li>
<li>Converting <code>EngineArgs</code> into a <code>VllmConfig</code>
</li>
<li>Decoding token ID <code>[29889]</code> back into the string <code>"."</code>
</li>
<li>Sending an <code>EngineCoreRequest</code> from the frontend process to the engine core process</li>
</ol>


Solution

1. **InputProcessor** â€” it runs the tokenizer on the raw prompt string.
2. **Scheduler** (inside the engine core) â€” it decides which requests to include in each step's batch.
3. **`LLMEngine.from_engine_args()`** â€” the factory classmethod calls `engine_args.create_engine_config()`.
4. **OutputProcessor** â€” it detokenizes raw token IDs back into text.
5. **EngineCoreClient** (multiprocess mode) â€” it serializes with `msgspec.msgpack` and sends over ZMQ.


<h3>
  
  
  Exercise 2: Multiprocess vs. In-Process
</h3>

<p><strong>Difficulty</strong>: Intermediate<br>
<strong>Goal</strong>: Understand when and why vLLM uses multiprocess communication</p>

<p>Consider two scenarios:</p>

<p><strong>Scenario A</strong>: Offline batch processing<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="n">llm</span> <span class="o">=</span> <span class="nc">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">meta-llama/Llama-3.1-8B-Instruct</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</code></pre>

</div>



<p><strong>Scenario B</strong>: Production API server<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight shell"><code>vllm serve meta-llama/Llama-3.1-8B-Instruct <span class="nt">--port</span> 8000
</code></pre>

</div>



<p>For each scenario:</p>

<ol>
<li>Is the <code>EngineCoreClient</code> in in-process or multiprocess mode?</li>
<li>Does the <code>EngineCoreRequest</code> actually get serialized with msgspec?</li>
<li>What would happen if the API server ran the engine core in-process (same event loop)?</li>
</ol>

<p>Solution</p>

<p><strong>Scenario A (offline <code>LLM</code> class)</strong>:</p>

<ol>
<li>
<strong>In-process</strong> â€” direct function calls to <code>EngineCore</code>.</li>
<li>
<strong>No</strong> â€” the <code>EngineCoreRequest</code> is created but passed directly without serialization.</li>
<li>N/A â€” there's no HTTP server.</li>
</ol>

<p><strong>Scenario B (API server)</strong>:</p>

<ol>
<li>
<strong>Multiprocess</strong> â€” ZMQ sockets between frontend and engine core processes.</li>
<li>
<strong>Yes</strong> â€” <code>msgspec.msgpack.encode()</code> serializes it, sends over ZMQ, and the core deserializes it.</li>
<li>The HTTP server would block during GPU forward passes. A single inference step can take 10-100ms, during which the server couldn't accept new connections or respond to health checks. Under load, this would cause request timeouts and dropped connections.</li>
</ol>

<h3>
  
  
  Exercise 3: Tracing Data Transformations
</h3>

<p><strong>Difficulty</strong>: Intermediate<br>
<strong>Goal</strong>: Follow the data as it changes form through the pipeline</p>

<p>Starting with this call:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="p">[{</span><span class="sh">"</span><span class="s">prompt_token_ids</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]}],</span>
    <span class="nc">SamplingParams</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="p">)</span>
</code></pre>

</div>



<ol>
<li>Does <code>InputProcessor</code> tokenize this prompt? Why or why not?</li>
<li>What fields of <code>EngineCoreRequest</code> are set? What's <code>arrival_time</code> used for?</li>
<li>If the model generates tokens <code>[100, 200, 300]</code>, what does the <code>EngineCoreOutput</code> for the final step look like?</li>
<li>What is <code>finish_reason</code> and why?</li>
</ol>

<p>Solution</p>

<ol>
<li>
<strong>No.</strong> The input is <code>{"prompt_token_ids": [1, 2, 3, 4, 5]}</code> â€” already tokenized. <code>InputProcessor</code> detects the <code>prompt_token_ids</code> key and skips tokenization, using the provided IDs directly.</li>
<li>Key fields: <code>request_id</code> (auto-assigned), <code>prompt_token_ids=[1, 2, 3, 4, 5]</code>, <code>sampling_params</code> (with <code>max_tokens=3, temperature=0</code>), <code>arrival_time=time.time()</code>. <code>arrival_time</code> is used by the scheduler for FCFS ordering and for latency metrics.</li>
<li>The final <code>EngineCoreOutput</code> would be: <code>EngineCoreOutput(request_id="0", new_token_ids=[300], finish_reason=FinishReason.LENGTH, stop_reason=None)</code>. Each step produces one new token, and the third token triggers the length limit.</li>
<li>
<strong><code>FinishReason.LENGTH</code></strong> â€” the model generated exactly <code>max_tokens=3</code> tokens (<code>[100, 200, 300]</code>) and was stopped. It didn't hit an EOS or stop token naturally.</li>
</ol>

<h3>
  
  
  Exercise 4: Design Challenge â€” Adding Request Priority
</h3>

<p><strong>Difficulty</strong>: Advanced<br>
<strong>Goal</strong>: Think through how a feature propagates through the engine layer</p>

<p>Suppose you want to add priority-based scheduling: high-priority requests should be processed before low-priority ones. Trace through the architecture:</p>

<ol>
<li>Where does the user specify priority? (Hint: look at <code>LLM.generate()</code> parameters)</li>
<li>How does priority get from the user to the scheduler? List each class it passes through.</li>
<li>Why is <code>priority</code> a field on <code>EngineCoreRequest</code> rather than just on <code>SamplingParams</code>?</li>
<li>What would happen if the <code>OutputProcessor</code> also needed to know about priority? Would the current architecture support that?</li>
</ol>

<p><strong>Hint</strong>: Priority is already partially implemented â€” look at the <code>EngineCoreRequest</code> fields.</p>

<p>Solution</p>

<ol>
<li>Via the <code>priority</code> parameter in <code>LLM.generate(prompts, params, priority=[1, 2, ...])</code>.</li>
<li>The path is: <code>LLM.generate()</code> â†’ <code>_validate_and_add_requests()</code> â†’ <code>LLMEngine.add_request()</code> â†’ <code>InputProcessor.process()</code> (sets the <code>priority</code> field on <code>EngineCoreRequest</code>) â†’ <code>EngineCoreClient.add_request()</code> â†’ <code>Scheduler</code> (reads <code>priority</code> from the request).</li>
<li>Priority is a <strong>request-level</strong> concept, not a <strong>generation-level</strong> concept. <code>SamplingParams</code> controls how tokens are sampled (temperature, top-p, etc.) â€” it's about the quality of the output. Priority controls when the request gets scheduled â€” it's about resource allocation. Mixing them would conflate two different concerns.</li>
<li>Yes â€” the <code>OutputProcessor</code> receives <code>EngineCoreOutput</code> which includes the <code>request_id</code>. It could look up priority from its internal state (it already maintains per-request <code>RequestState</code>). But currently it doesn't need to â€” priority only matters for scheduling decisions.</li>
</ol>

<h3>
  
  
  Exercise 5: Streaming Output Modes
</h3>

<p><strong>Difficulty</strong>: Advanced<br>
<strong>Goal</strong>: Understand the difference between CUMULATIVE and DELTA output modes</p>

<p>Given a request that generates the text <code>"Hello world!"</code> as three tokens:</p>

<ul>
<li>Step 1: token <code>"Hello"</code>
</li>
<li>Step 2: token <code>" world"</code>
</li>
<li>Step 3: token <code>"!"</code>
</li>
</ul>

<p>Write out what <code>RequestOutput.outputs[0].text</code> contains at each step for:</p>

<ol>
<li><code>output_kind = RequestOutputKind.CUMULATIVE</code></li>
<li><code>output_kind = RequestOutputKind.DELTA</code></li>
</ol>

<p>When would you use each mode? Think about a streaming chat UI vs. a batch processing pipeline.</p>

<p>Solution</p>

<p><strong>CUMULATIVE</strong>:</p>

<ul>
<li>Step 1: <code>"Hello"</code>
</li>
<li>Step 2: <code>"Hello world"</code>
</li>
<li>Step 3: <code>"Hello world!"</code>
</li>
</ul>

<p><strong>DELTA</strong>:</p>

<ul>
<li>Step 1: <code>"Hello"</code>
</li>
<li>Step 2: <code>" world"</code>
</li>
<li>Step 3: <code>"!"</code>
</li>
</ul>

<p><strong>When to use each</strong>: DELTA is ideal for streaming chat UIs â€” you append each delta directly to the display. CUMULATIVE is simpler for batch pipelines â€” you always have the full text so far, no need to track previous outputs. CUMULATIVE is the default because it's easier to use correctly.</p>




<h2>
  
  
  Quiz
</h2>

<p>Answer these questions based on today's material. Try to answer each question before revealing the answer.</p>

<p><strong>Q1: What are the three main components inside <code>LLMEngine</code>, and what does each one do?</strong></p>

<p>Answer</p>

<p><code>InputProcessor</code>, <code>EngineCoreClient</code>, and <code>OutputProcessor</code>. <code>InputProcessor</code> tokenizes prompts and creates <code>EngineCoreRequest</code> objects. <code>EngineCoreClient</code> sends requests to and receives outputs from the engine core (either in-process or via ZMQ). <code>OutputProcessor</code> detokenizes raw token IDs back into text and formats <code>RequestOutput</code> objects for the user.</p>

<p><strong>Q2: Why does vLLM have both <code>EngineCoreRequest</code> and <code>Request</code> as separate types?</strong></p>

<p>Answer</p>

<p>They serve different purposes across a process boundary. <code>EngineCoreRequest</code> is the transport/wire format â€” immutable, serializable with <code>msgspec</code>, designed to cross process boundaries efficiently. <code>Request</code> is the scheduler's internal runtime format â€” mutable, tracks state like <code>num_computed_tokens</code>, allocated KV cache blocks, and output tokens. Mixing these concerns would either make serialization expensive or make runtime tracking awkward.</p>

<p><strong>Q3: What serialization format does vLLM use for inter-process communication, and why was it chosen over alternatives like pickle or JSON?</strong></p>

<p>Answer</p>

<p><code>msgspec.msgpack</code> â€” a binary MessagePack format. It's 10-50x faster than pickle for structured data and produces compact binary payloads. JSON was rejected because it's text-based (larger payloads, slower parsing). Pickle was rejected because it's slow for structured data and has security concerns. At thousands of requests per second, serialization overhead is a real bottleneck.</p>

<p><strong>Q4: In multiprocess mode, what happens if the engine core is busy running a forward pass when a new HTTP request arrives?</strong></p>

<p>Answer</p>

<p>The new request is accepted by the frontend process and queued. Because the frontend (FastAPI + <code>InputProcessor</code>) runs in a separate process from the engine core, it can accept and preprocess new HTTP requests while the GPU is busy. The <code>EngineCoreRequest</code> is sent over ZMQ and queued for the next scheduling iteration. This is exactly why the two-process architecture exists.</p>

<p><strong>Q5: What does <code>OutputProcessor</code> do when it receives a token that represents an incomplete UTF-8 character?</strong></p>

<p>Answer</p>

<p>It buffers the incomplete bytes until a valid character is formed. Many tokenizers use byte-level BPE, where tokens can split in the middle of multi-byte UTF-8 characters (e.g., emoji, CJK characters). The <code>OutputProcessor</code> accumulates bytes and only emits text when complete characters are available. This prevents garbled output in streaming responses.</p>

<p><strong>Q6: True or false: In in-process mode (using the <code>LLM</code> class), <code>EngineCoreRequest</code> is still created even though it doesn't need to be serialized.</strong></p>

<p>Answer</p>

<p>True. The <code>InputProcessor</code> always creates an <code>EngineCoreRequest</code> regardless of execution mode. In in-process mode, the request is passed directly to the engine core without serialization. The <code>EngineCoreRequest</code> type serves as a clean interface contract between the engine layer and the core, even when no process boundary exists.</p>

<p><strong>Q7: What is the purpose of <code>arrival_time</code> in <code>EngineCoreRequest</code>?</strong></p>

<p>Answer</p>

<p>It records when the request was submitted, enabling scheduling policies like FCFS (first-come-first-served). The scheduler can use <code>arrival_time</code> to prioritize older requests over newer ones. It's also used for metrics: you can measure end-to-end latency by comparing <code>arrival_time</code> with the completion time. Without it, the scheduler would have no notion of fairness or request ordering.</p>

<p><strong>Q8: Why does <code>LLMEngine.from_engine_args()</code> exist as a classmethod factory instead of putting all the logic in <code>__init__</code>?</strong></p>

<p>Answer</p>

<p>To separate argument parsing from construction. The factory method converts user-friendly <code>EngineArgs</code> (flat key-value pairs) into a structured <code>VllmConfig</code> (nested, validated configuration), selects the right executor class, and then calls <code>__init__</code>. This keeps <code>__init__</code> simple â€” it receives fully validated, structured objects. It also allows alternative construction paths (e.g., creating <code>LLMEngine</code> directly with a <code>VllmConfig</code> for testing).</p>




<h2>
  
  
  Summary
</h2>

<ul>
<li>
<strong><code>LLMEngine</code></strong> is the orchestrator that connects the user-facing API to the engine core, with three sub-components: <code>InputProcessor</code>, <code>EngineCoreClient</code>, and <code>OutputProcessor</code>
</li>
<li>
<strong><code>InputProcessor</code></strong> normalizes various input formats (strings, token IDs, multimodal data) into <code>EngineCoreRequest</code> â€” the standard wire format</li>
<li>
<strong><code>EngineCoreRequest</code></strong> uses <code>msgspec.Struct</code> for fast serialization, enabling efficient multiprocess communication via ZMQ</li>
<li>
<strong><code>EngineCoreClient</code></strong> abstracts the communication mode: in-process for offline use, multiprocess (ZMQ) for production servers</li>
<li>
<strong><code>OutputProcessor</code></strong> reverses the input pipeline: accumulates tokens, detokenizes, handles streaming modes (CUMULATIVE vs DELTA), and produces <code>RequestOutput</code>
</li>
<li>The <strong>two-process architecture</strong> (frontend + engine core) is critical for production: it keeps the HTTP server responsive while the GPU runs inference</li>
<li>Next session: <strong>The Scheduler</strong> â€” how vLLM decides which requests get GPU time, the token budget system, and chunked prefill</li>
</ul>




<p><em>Generated from my <a href="https://github.com/ccwang/ai-study" rel="noopener noreferrer">ai-study</a> learning project.</em></p>

