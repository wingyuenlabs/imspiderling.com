---
Title: Session 1: vLLM Overview and the User API
Description: 
Author: Ben
Date: 2026-02-01T22:00:01.000Z
Robots: noindex,nofollow
Template: index
---
<p><em>This is part of my vLLM learning series. In this session, I cover Step 1 (The User API).</em></p>

<blockquote>
<p><strong>Note</strong>: This content was generated by Claude, grounded on the actual<br>
<a href="https://github.com/vllm-project/vllm" rel="noopener noreferrer">vLLM</a> codebase. It is intended for personal<br>
learning only and may contain inaccuracies. Always verify against the<br>
original source code and official documentation.</p>
</blockquote>

<p><strong>Topic</strong>: vLLM<br>
<strong>Date</strong>: 2026-01-31<br>
<strong>Sections covered</strong>: Step 1 (The User API)<br>
<strong>Prerequisites</strong>: None</p>


<h2>
  
  
  Today's Material
</h2>
<h3>
  
  
  1. What is vLLM and Why Does It Matter?
</h3>

<p>LLM inference is GPU-memory-bound. When a model generates text, it needs to store <strong>key-value (KV) caches</strong> â€” intermediate computations from the attention mechanism â€” for every token in every active request. Naive implementations pre-allocate the maximum possible sequence length for each request, wasting 60-80% of GPU memory on empty space.</p>

<p>vLLM solves this with <strong>PagedAttention</strong>: instead of pre-allocating a giant contiguous buffer per request, it carves GPU memory into fixed-size <strong>blocks</strong> (default 16 tokens each) and allocates them on demand â€” just like how an operating system manages virtual memory with pages.</p>

<p>The result: near-optimal memory utilization and <strong>2-4x higher throughput</strong> than HuggingFace Transformers on typical workloads.</p>

<blockquote>
<p><strong>ğŸ“ Note:</strong><br>
Think of the difference like this: the naive approach is like reserving an entire row of seats in a theater for each person "just in case" they bring friends. PagedAttention is like assigning individual seats as people actually show up.</p>
</blockquote>
<h3>
  
  
  2. High-Level Architecture
</h3>

<p>Before diving into code, here's the bird's-eye view of how vLLM is organized:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              User-Facing Layer                â”‚
â”‚   LLM class  |  OpenAI API Server  |  gRPC   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Engine Layer                     â”‚
â”‚  InputProcessor â†’ EngineCoreClient â†’ OutputProcessor â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Engine Core                      â”‚
â”‚   Scheduler â†’ Executor â†’ Workers â†’ GPU        â”‚
â”‚      â””â”€â”€ KVCacheManager (BlockPool)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>

</div>



<p>Three layers:</p>

<ol>
<li>
<strong>User-Facing</strong> â€” Multiple entry points (Python API, HTTP, gRPC) that all funnel into the engine</li>
<li>
<strong>Engine Layer</strong> â€” Tokenize inputs, relay to the core, format outputs</li>
<li>
<strong>Engine Core</strong> â€” The scheduling loop, KV cache management, and GPU execution</li>
</ol>

<p>Today we focus on layer 1: the <code>LLM</code> class and its associated types.</p>

<h3>
  
  
  3. The LLM Class â€” Your Main Interface
</h3>

<p>The <code>LLM</code> class in <code>vllm/entrypoints/llm.py</code> is the primary interface for offline batch inference. Here's its constructor (simplified to the most important parameters):<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/entrypoints/llm.py
</span><span class="k">class</span> <span class="nc">LLM</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">tensor_parallel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">ModelDType</span> <span class="o">=</span> <span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">quantization</span><span class="p">:</span> <span class="n">QuantizationMethods</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">gpu_memory_utilization</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">engine_args</span> <span class="o">=</span> <span class="nc">EngineArgs</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
            <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="n">tensor_parallel_size</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">quantization</span><span class="o">=</span><span class="n">quantization</span><span class="p">,</span>
            <span class="n">gpu_memory_utilization</span><span class="o">=</span><span class="n">gpu_memory_utilization</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">llm_engine</span> <span class="o">=</span> <span class="n">LLMEngine</span><span class="p">.</span><span class="nf">from_engine_args</span><span class="p">(</span>
            <span class="n">engine_args</span><span class="o">=</span><span class="n">engine_args</span><span class="p">,</span>
            <span class="n">usage_context</span><span class="o">=</span><span class="n">UsageContext</span><span class="p">.</span><span class="n">LLM_CLASS</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">request_counter</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">()</span>
</code></pre>

</div>



<p>Key things to notice:</p>

<ul>
<li>
<code>LLM</code> is <strong>thin</strong> â€” it creates an <code>EngineArgs</code> config, then hands everything off to <code>LLMEngine.from_engine_args()</code>
</li>
<li>
<code>gpu_memory_utilization=0.9</code> means vLLM claims 90% of GPU memory for the KV cache, reserving 10% for PyTorch overhead (model weights, activations, etc.)</li>
<li>
<code>tensor_parallel_size</code> controls how many GPUs to shard the model across â€” set to 1 for single-GPU</li>
</ul>

<blockquote>
<p><strong>ğŸ’¡ Tip:</strong><br>
If you get CUDA out-of-memory errors, lower <code>gpu_memory_utilization</code> (e.g., to 0.8). If you want more throughput and have headroom, raise it (up to ~0.95).</p>
</blockquote>

<h3>
  
  
  4. The generate() Method â€” Where Requests Enter
</h3>



<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/entrypoints/llm.py
</span><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span>
    <span class="n">prompts</span><span class="p">:</span> <span class="n">PromptType</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">PromptType</span><span class="p">],</span>
    <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span> <span class="o">|</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">SamplingParams</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">use_tqdm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">[...,</span> <span class="n">tqdm</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
    <span class="n">lora_request</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">LoRARequest</span><span class="p">]</span> <span class="o">|</span> <span class="n">LoRARequest</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">priority</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">]:</span>
    <span class="n">model_config</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model_config</span>
    <span class="n">runner_type</span> <span class="o">=</span> <span class="n">model_config</span><span class="p">.</span><span class="n">runner_type</span>
    <span class="k">if</span> <span class="n">runner_type</span> <span class="o">!=</span> <span class="sh">"</span><span class="s">generate</span><span class="sh">"</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span>
            <span class="sh">"</span><span class="s">LLM.generate() is only supported for generative models.</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sampling_params</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">sampling_params</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">get_default_sampling_params</span><span class="p">()</span>

    <span class="n">self</span><span class="p">.</span><span class="nf">_validate_and_add_requests</span><span class="p">(</span>
        <span class="n">prompts</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span>
        <span class="n">params</span><span class="o">=</span><span class="n">sampling_params</span><span class="p">,</span>
        <span class="n">use_tqdm</span><span class="o">=</span><span class="n">use_tqdm</span><span class="p">,</span>
        <span class="n">lora_request</span><span class="o">=</span><span class="p">...,</span>
        <span class="n">priority</span><span class="o">=</span><span class="n">priority</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_run_engine</span><span class="p">(</span><span class="n">use_tqdm</span><span class="o">=</span><span class="n">use_tqdm</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">engine_class</span><span class="p">.</span><span class="nf">validate_outputs</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">RequestOutput</span><span class="p">)</span>
</code></pre>

</div>



<p>The flow is:</p>

<ol>
<li>
<strong>Validate</strong> that this is a generative model (not an embedding model)</li>
<li>
<strong>Add requests</strong> via <code>_validate_and_add_requests()</code> â€” normalizes inputs, pairs each prompt with its <code>SamplingParams</code>, and sends them to the engine</li>
<li>
<strong>Run the engine</strong> via <code>_run_engine()</code> â€” loops until all requests are finished</li>
<li>
<strong>Return</strong> sorted <code>RequestOutput</code> objects</li>
</ol>

<p>You can pass a single <code>SamplingParams</code> (applied to all prompts) or a list (one per prompt). This is useful when different prompts need different temperatures or stop conditions.</p>

<h3>
  
  
  5. _run_engine() â€” The Processing Loop
</h3>

<p>This is where the actual inference happens:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/entrypoints/llm.py
</span><span class="k">def</span> <span class="nf">_run_engine</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">use_tqdm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">|</span> <span class="n">Callable</span><span class="p">[...,</span> <span class="n">tqdm</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">RequestOutput</span> <span class="o">|</span> <span class="n">PoolingRequestOutput</span><span class="p">]:</span>
    <span class="n">outputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">RequestOutput</span> <span class="o">|</span> <span class="n">PoolingRequestOutput</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_in_toks</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_out_toks</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="n">self</span><span class="p">.</span><span class="n">llm_engine</span><span class="p">.</span><span class="nf">has_unfinished_requests</span><span class="p">():</span>
        <span class="n">step_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">llm_engine</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">step_outputs</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">output</span><span class="p">.</span><span class="n">finished</span><span class="p">:</span>
                <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="c1"># Sort the outputs by request ID.
</span>    <span class="c1"># This is necessary because some requests may be finished earlier than
</span>    <span class="c1"># its previous requests.
</span>    <span class="k">return</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">int</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">request_id</span><span class="p">))</span>
</code></pre>

</div>



<p>The key insight: <strong><code>_run_engine()</code> is a simple loop</strong>. It calls <code>self.llm_engine.step()</code> repeatedly. Each <code>step()</code> runs one iteration of the scheduling + inference pipeline â€” potentially processing hundreds of requests in a single forward pass. Finished requests come back as <code>RequestOutput</code> objects.</p>

<blockquote>
<p><strong>âš ï¸ Warning:</strong><br>
The outputs are sorted by <code>request_id</code> at the end because requests don't finish in order. A short request (e.g., "Say hi") may finish in 5 iterations while a long request (e.g., "Write an essay") takes 500. The sorting ensures the output list matches the input prompt order.</p>
</blockquote>

<p><strong>Why this matters</strong>: This loop is where continuous batching happens. Unlike static batching (process N prompts, wait for all to finish, return), vLLM processes requests at different stages simultaneously. Request A might be mid-generation while Request B is just starting its prefill.</p>

<h3>
  
  
  6. SamplingParams â€” Controlling Generation
</h3>

<p>Every request carries a <code>SamplingParams</code> that controls how tokens are selected:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/sampling_params.py
</span><span class="k">class</span> <span class="nc">SamplingParams</span><span class="p">(</span>
    <span class="n">PydanticMsgspecMixin</span><span class="p">,</span>
    <span class="n">msgspec</span><span class="p">.</span><span class="n">Struct</span><span class="p">,</span>
    <span class="n">omit_defaults</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="nb">dict</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># --- Core sampling ---
</span>    <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>                          <span class="c1"># Number of output sequences
</span>    <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>            <span class="c1"># 0 = greedy, higher = more random
</span>    <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>                  <span class="c1"># Nucleus sampling threshold
</span>    <span class="n">top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>                      <span class="c1"># Top-K filtering (0 = disabled)
</span>    <span class="n">min_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>                  <span class="c1"># Minimum probability threshold
</span>    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span>             <span class="c1"># Reproducible sampling
</span>
    <span class="c1"># --- Penalties ---
</span>    <span class="n">presence_penalty</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>       <span class="c1"># Penalize tokens that appeared
</span>    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>      <span class="c1"># Penalize by frequency
</span>    <span class="n">repetition_penalty</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>     <span class="c1"># Multiplicative penalty
</span>
    <span class="c1"># --- Generation limits ---
</span>    <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="mi">16</span>         <span class="c1"># Output length limit
</span>    <span class="n">min_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>                 <span class="c1"># Minimum before allowing EOS
</span>    <span class="n">ignore_eos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>            <span class="c1"># Don't stop at EOS
</span>
    <span class="c1"># --- Stop conditions ---
</span>    <span class="n">stop</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">stop_token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="c1"># --- Output control ---
</span>    <span class="n">logprobs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span>         <span class="c1"># Return top-N log probabilities
</span>    <span class="n">prompt_logprobs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c1"># Prompt token log probs
</span>    <span class="n">detokenize</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span>             <span class="c1"># Decode token IDs to text
</span>
    <span class="c1"># --- Advanced ---
</span>    <span class="n">structured_outputs</span><span class="p">:</span> <span class="n">StructuredOutputsParams</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c1"># JSON schema
</span>    <span class="n">logit_bias</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">output_kind</span><span class="p">:</span> <span class="n">RequestOutputKind</span> <span class="o">=</span> <span class="n">RequestOutputKind</span><span class="p">.</span><span class="n">CUMULATIVE</span>
</code></pre>

</div>



<p>Notice that <code>SamplingParams</code> inherits from <code>msgspec.Struct</code>, not a Python dataclass. This is a deliberate performance choice â€” <code>msgspec</code> serialization is 10-50x faster than <code>pickle</code>, which matters when requests cross process boundaries (more on this in a future session).</p>

<h4>
  
  
  Validation logic
</h4>

<p><code>SamplingParams.__post_init__()</code> enforces constraints:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/sampling_params.py
</span><span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Normalize stop to a list
</span>    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">stop</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">stop</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">elif</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">stop</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">stop</span> <span class="o">=</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">stop</span><span class="p">]</span>

    <span class="c1"># Zero temperature â†’ force greedy sampling
</span>    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">temperature</span> <span class="o">&lt;</span> <span class="n">_SAMPLING_EPS</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">top_p</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">top_k</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">min_p</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">_verify_greedy_sampling</span><span class="p">()</span>

    <span class="n">self</span><span class="p">.</span><span class="nf">_verify_args</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">_verify_args</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">n</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">n must be at least 1, got </span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">n</span><span class="si">}</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="o">-</span><span class="mf">2.0</span> <span class="o">&lt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">presence_penalty</span> <span class="o">&lt;=</span> <span class="mf">2.0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(...)</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">temperature</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">VLLMValidationError</span><span class="p">(...)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">top_p</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">VLLMValidationError</span><span class="p">(...)</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">max_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">self</span><span class="p">.</span><span class="n">max_tokens</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">VLLMValidationError</span><span class="p">(...)</span>
</code></pre>

</div>



<blockquote>
<p><strong>ğŸ“ Note:</strong><br>
When <code>temperature=0</code>, vLLM automatically sets <code>top_p=1.0</code>, <code>top_k=0</code>, and <code>min_p=0.0</code>. This is because greedy decoding (always pick the highest-probability token) makes all other sampling parameters irrelevant. The code enforces this rather than letting the user set contradictory values.</p>
</blockquote>

<h3>
  
  
  7. RequestOutput and CompletionOutput â€” What You Get Back
</h3>

<p>After <code>generate()</code> finishes, you get a list of <code>RequestOutput</code> objects:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/outputs.py
</span><span class="k">class</span> <span class="nc">RequestOutput</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">request_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">|</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">prompt_logprobs</span><span class="p">:</span> <span class="n">PromptLogprobs</span> <span class="o">|</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">CompletionOutput</span><span class="p">],</span>
        <span class="n">finished</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">metrics</span><span class="p">:</span> <span class="n">RequestStateStats</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">num_cached_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="bp">None</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="bp">...</span>
    <span class="p">):</span> <span class="bp">...</span>
</code></pre>

</div>



<p>Each <code>RequestOutput</code> contains one or more <code>CompletionOutput</code> objects (one per <code>n</code> in <code>SamplingParams</code>):<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># vllm/outputs.py
</span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">CompletionOutput</span><span class="p">:</span>
    <span class="n">index</span><span class="p">:</span> <span class="nb">int</span>                         <span class="c1"># Which of the n outputs
</span>    <span class="n">text</span><span class="p">:</span> <span class="nb">str</span>                          <span class="c1"># Generated text
</span>    <span class="n">token_ids</span><span class="p">:</span> <span class="n">GenericSequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>    <span class="c1"># Generated token IDs
</span>    <span class="n">cumulative_logprob</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="bp">None</span>   <span class="c1"># Sum of log probs
</span>    <span class="n">logprobs</span><span class="p">:</span> <span class="n">SampleLogprobs</span> <span class="o">|</span> <span class="bp">None</span>    <span class="c1"># Per-token log probs
</span>    <span class="n">finish_reason</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="bp">None</span>          <span class="c1"># "stop", "length", or None
</span>    <span class="n">stop_reason</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="bp">None</span>      <span class="c1"># What triggered the stop
</span>
    <span class="k">def</span> <span class="nf">finished</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">finish_reason</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
</code></pre>

</div>



<p>A typical usage pattern:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="kn">from</span> <span class="n">vllm</span> <span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">meta-llama/Llama-3.1-8B-Instruct</span><span class="sh">"</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">What is the capital of France?</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Explain gravity in one sentence.</span><span class="sh">"</span><span class="p">],</span>
    <span class="nc">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">prompt</span>
    <span class="n">generated</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span>
    <span class="n">reason</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">finish_reason</span>  <span class="c1"># "stop" or "length"
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Output: </span><span class="si">{</span><span class="n">generated</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Finished because: </span><span class="si">{</span><span class="n">reason</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre>

</div>



<p>When <code>n &gt; 1</code>, you get multiple completions per prompt:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">Tell me a joke.</span><span class="sh">"</span><span class="p">],</span>
    <span class="nc">SamplingParams</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># outputs[0].outputs has 3 CompletionOutput objects
</span><span class="k">for</span> <span class="n">completion</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">outputs</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Completion </span><span class="si">{</span><span class="n">completion</span><span class="p">.</span><span class="n">index</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">completion</span><span class="p">.</span><span class="n">text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre>

</div>



<h3>
  
  
  8. Beyond generate() â€” Other Task Types
</h3>

<p>The <code>LLM</code> class supports more than text generation:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="c1"># Chat (applies chat template automatically)
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is 2+2?</span><span class="sh">"</span><span class="p">}],</span>
    <span class="n">sampling_params</span><span class="o">=</span><span class="nc">SamplingParams</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Embeddings
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">embed</span><span class="p">([</span><span class="sh">"</span><span class="s">Hello world</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Goodbye world</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># Classification (not all models support this)
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">classify</span><span class="p">([</span><span class="sh">"</span><span class="s">This movie was great!</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Terrible film.</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># Scoring (cross-encoder style)
</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="sh">"</span><span class="s">query text</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="sh">"</span><span class="s">doc1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">doc2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">doc3</span><span class="sh">"</span><span class="p">])</span>
</code></pre>

</div>



<p>Each method validates that the loaded model supports the requested task via <code>runner_type</code>. If you try <code>llm.generate()</code> on an embedding model, you get a clear error.</p>




<h2>
  
  
  Exercises
</h2>

<h3>
  
  
  Exercise 1: Basic Generation
</h3>

<p><strong>Difficulty</strong>: Beginner<br>
<strong>Goal</strong>: Understand the relationship between <code>SamplingParams</code> and output</p>

<p>Given this code:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="n">llm</span> <span class="o">=</span> <span class="nc">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">meta-llama/Llama-3.1-8B-Instruct</span><span class="sh">"</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="nc">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">([</span><span class="sh">"</span><span class="s">Count from 1 to 10.</span><span class="sh">"</span><span class="p">],</span> <span class="n">params</span><span class="p">)</span>
</code></pre>

</div>



<ol>
<li>What happens when <code>temperature=0</code> and <code>n=2</code>? Will the two completions be different or identical? Why?</li>
<li>What will <code>finish_reason</code> be for each completion? (<code>"stop"</code> or <code>"length"</code>)</li>
<li>How many <code>CompletionOutput</code> objects will be in <code>outputs[0].outputs</code>?</li>
</ol>

<p><strong>Hint</strong>: Think about what greedy decoding means for multiple samples.</p>

<p>Solution</p>

<ol>
<li>
<strong>Identical.</strong> Temperature=0 means greedy decoding â€” always pick the highest-probability token. With no randomness, every sample produces the exact same sequence. Running <code>n=2</code> with greedy is wasteful.</li>
<li>
<strong><code>"length"</code></strong> for both. <code>max_tokens=5</code> will cut off "Count from 1 to 10" well before the model naturally stops â€” it would need at least ~20 tokens ("1, 2, 3, 4, 5, 6, 7, 8, 9, 10").</li>
<li>
<strong>2</strong> â€” one per <code>n</code>. <code>outputs[0].outputs[0]</code> and <code>outputs[0].outputs[1]</code>, though both will have the same text.</li>
</ol>

<h3>
  
  
  Exercise 2: Trace the Call Path
</h3>

<p><strong>Difficulty</strong>: Intermediate<br>
<strong>Goal</strong>: Map the execution flow from user call to engine loop</p>

<p>Trace what happens when this code executes:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight python"><code><span class="n">llm</span> <span class="o">=</span> <span class="nc">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">meta-llama/Llama-3.1-8B-Instruct</span><span class="sh">"</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="p">[</span><span class="sh">"</span><span class="s">Hello</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">World</span><span class="sh">"</span><span class="p">],</span>
    <span class="nc">SamplingParams</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
</code></pre>

</div>



<p>For each step, name the method and describe what it does:</p>

<ol>
<li>What does <code>generate()</code> call first?</li>
<li>How are the two prompts and the single <code>SamplingParams</code> paired?</li>
<li>What does <code>_run_engine()</code> do on each iteration?</li>
<li>Why are outputs sorted at the end?</li>
</ol>

<p>Solution</p>

<ol>
<li>
<code>generate()</code> first validates the runner type is <code>"generate"</code>, then calls <code>_validate_and_add_requests()</code>.</li>
<li>The single <code>SamplingParams</code> is replicated: <code>[params] * num_requests</code> â€” so both "Hello" and "World" get the same <code>max_tokens=10</code>.</li>
<li>Each iteration calls <code>self.llm_engine.step()</code>, which runs one scheduling + inference cycle. Finished requests are collected into the <code>outputs</code> list.</li>
<li>Because requests finish out of order. "Hello" (shorter) might finish before "World" (or vice versa depending on generation). Sorting by <code>request_id</code> ensures <code>outputs[0]</code> corresponds to "Hello" and <code>outputs[1]</code> to "World".</li>
</ol>

<h3>
  
  
  Exercise 3: SamplingParams Edge Cases
</h3>

<p><strong>Difficulty</strong>: Intermediate<br>
<strong>Goal</strong>: Understand validation and normalization</p>

<p>What happens in each case? Does it succeed, raise an error, or get silently normalized?</p>

<ol>
<li><code>SamplingParams(temperature=-0.5)</code></li>
<li><code>SamplingParams(temperature=0, top_k=50)</code></li>
<li><code>SamplingParams(top_p=0.0)</code></li>
<li>
<code>SamplingParams(stop="END", include_stop_str_in_output=False)</code> â€” what does <code>output_text_buffer_length</code> get set to?</li>
<li><code>SamplingParams(max_tokens=0)</code></li>
</ol>

<p>Solution</p>

<ol>
<li>
<strong>Raises <code>VLLMValidationError</code></strong> â€” <code>_verify_args()</code> checks <code>self.temperature &amp;lt; 0.0</code>.</li>
<li>
<strong>Silently normalized.</strong> When temperature=0, <code>__post_init__</code> forces <code>top_k=0</code> (along with <code>top_p=1.0</code>, <code>min_p=0.0</code>). Your <code>top_k=50</code> is overwritten.</li>
<li>
<strong>Raises <code>VLLMValidationError</code></strong> â€” <code>_verify_args()</code> checks <code>not 0.0 &amp;lt; self.top_p &amp;lt;= 1.0</code>. Zero is not in the valid range.</li>
<li>
<strong><code>output_text_buffer_length</code> is set to <code>len("END") - 1 = 2</code>.</strong> This buffer ensures the output processor doesn't emit text that might be part of the stop string before the full match is determined.</li>
<li>
<strong>Raises <code>VLLMValidationError</code></strong> â€” <code>_verify_args()</code> checks <code>self.max_tokens &amp;lt; 1</code> when not None.</li>
</ol>

<h3>
  
  
  Exercise 4: Design a Batch Inference Script
</h3>

<p><strong>Difficulty</strong>: Advanced<br>
<strong>Goal</strong>: Apply what you've learned to a realistic scenario</p>

<p>You have a file with 10,000 prompts (one per line). You need to generate completions with <code>temperature=0.8</code> and <code>max_tokens=256</code>, saving results to a JSON file. Design the script:</p>

<ol>
<li>Should you call <code>generate()</code> once with all 10,000 prompts, or in batches of 100? Why?</li>
<li>How would you handle prompts that need different <code>max_tokens</code>?</li>
<li>If 3 out of 10,000 prompts fail, how would you know which ones? (Hint: look at <code>request_id</code>)</li>
</ol>

<p>Solution</p>

<ol>
<li>
<strong>Call <code>generate()</code> once with all 10,000.</strong> vLLM's continuous batching handles scheduling internally â€” it dynamically fits as many requests as GPU memory allows per step. Breaking into batches of 100 would serialize work unnecessarily and prevent vLLM from optimally utilizing the GPU.</li>
<li>
<strong>Pass a list of <code>SamplingParams</code></strong>, one per prompt: <code>[SamplingParams(max_tokens=t) for t in per_prompt_max_tokens]</code>. This lets each prompt have its own configuration.</li>
<li>
<strong>Match by index.</strong> Since <code>generate()</code> sorts outputs by <code>request_id</code> (which maps to the input order), <code>outputs[i]</code> corresponds to <code>prompts[i]</code>. Check <code>outputs[i].outputs[0].finish_reason</code> â€” if it's <code>None</code> or shows an unexpected state, that prompt had issues. You could also check <code>len(outputs)</code> vs <code>len(prompts)</code> to see if any were dropped.</li>
</ol>




<h2>
  
  
  Quiz
</h2>

<p>Answer these questions based on today's material. Try to answer each question before revealing the answer.</p>

<p><strong>Q1: What does <code>LLM.__init__()</code> actually do with its parameters? Where does the heavy lifting happen?</strong></p>

<p>Answer</p>

<p>It packs parameters into <code>EngineArgs</code> and calls <code>LLMEngine.from_engine_args()</code>. The <code>LLM</code> class itself does minimal work â€” it's a convenience wrapper. The engine factory method parses the args into a <code>VllmConfig</code>, selects the right executor, loads the model, allocates the KV cache, and initializes the scheduler.</p>

<p><strong>Q2: Why does <code>_run_engine()</code> sort its outputs by <code>request_id</code> before returning?</strong></p>

<p>Answer</p>

<p>Because requests don't finish in order. Short prompts complete in fewer iterations than long ones. Since <code>_run_engine()</code> collects outputs as they finish, a request with <code>request_id=5</code> might finish before <code>request_id=3</code>. Sorting by request ID restores the original prompt order so <code>outputs[i]</code> corresponds to <code>prompts[i]</code>.</p>

<p><strong>Q3: What is the default value of <code>max_tokens</code> in <code>SamplingParams</code>, and why might this surprise users?</strong></p>

<p>Answer</p>

<p>The default is 16 tokens. This is much smaller than most users expect (GPT-4 defaults to ~4096). If your outputs seem cut short, you probably need to set <code>max_tokens</code> explicitly. The low default is intentional â€” it prevents accidental resource exhaustion when experimenting.</p>

<p><strong>Q4: What happens internally to <code>SamplingParams</code> when you set <code>temperature=0</code>?</strong></p>

<p>Answer</p>

<p>vLLM forces greedy sampling parameters. Specifically, it sets <code>top_p=1.0</code>, <code>top_k=0</code>, and <code>min_p=0.0</code>, then calls <code>_verify_greedy_sampling()</code>. This is because when temperature is zero (always pick the highest-probability token), top-p/top-k filtering is meaningless and could introduce unexpected behavior.</p>

<p><strong>Q5: Why does <code>SamplingParams</code> inherit from <code>msgspec.Struct</code> instead of using a Python <code>@dataclass</code>?</strong></p>

<p>Answer</p>

<p>Serialization performance. <code>msgspec.Struct</code> provides 10-50x faster serialization than pickle (used with dataclasses). This matters because when vLLM runs in multiprocess mode, <code>SamplingParams</code> is serialized with <code>msgspec.msgpack</code> and sent over ZMQ sockets from the frontend process to the engine core process. Faster serialization = lower per-request overhead.</p>

<p><strong>Q6: What is the difference between <code>finish_reason="stop"</code> and <code>finish_reason="length"</code> in <code>CompletionOutput</code>?</strong></p>

<p>Answer</p>

<p><code>"stop"</code> means the model naturally stopped â€” it hit an EOS token, a stop string, or a stop token ID. <code>"length"</code> means it hit <code>max_tokens</code> â€” the model wanted to keep generating but was cut off. If you see many <code>"length"</code> finishes, consider increasing <code>max_tokens</code>.</p>

<p><strong>Q7: True or false: Calling <code>llm.generate()</code> with a single <code>SamplingParams</code> and a list of 100 prompts will use the same sampling parameters for all 100 prompts.</strong></p>

<p>Answer</p>

<p>True. When you pass a single <code>SamplingParams</code> (not a list), <code>_validate_and_add_requests()</code> replicates it: <code>engine_params = [params] * num_requests</code>. Each prompt gets the same sampling configuration. To use different parameters per prompt, pass a list of <code>SamplingParams</code> with the same length as the prompts list.</p>

<p><strong>Q8: What does <code>gpu_memory_utilization=0.9</code> mean, and what happens to the other 10%?</strong></p>

<p>Answer</p>

<p>vLLM uses 90% of GPU memory for the KV cache (and model weights). The remaining 10% is reserved for PyTorch's internal allocations â€” temporary activation tensors, CUDA context, cuBLAS workspace, etc. If you set it too high (e.g., 0.99), you risk CUDA OOM during forward passes. If you set it too low, you waste GPU capacity.</p>




<h2>
  
  
  Summary
</h2>

<ul>
<li>
<strong>vLLM</strong> solves the KV cache memory waste problem via PagedAttention â€” block-based allocation instead of pre-allocation</li>
<li>The <strong>LLM</strong> class is a thin wrapper: it creates <code>EngineArgs</code>, builds <code>LLMEngine</code>, and provides <code>generate()</code>, <code>chat()</code>, <code>embed()</code>, and other task methods</li>
<li>
<strong><code>generate()</code></strong> validates inputs, adds requests to the engine, then loops <code>step()</code> until all requests finish</li>
<li>
<strong><code>_run_engine()</code></strong> is a simple while-loop over <code>llm_engine.step()</code> â€” this is where continuous batching happens under the hood</li>
<li>
<strong><code>SamplingParams</code></strong> controls per-request generation with thorough validation â€” zero temperature forces greedy mode, invalid ranges raise errors</li>
<li>
<strong><code>RequestOutput</code></strong> wraps one or more <code>CompletionOutput</code> objects, each containing the generated text, token IDs, and finish reason</li>
<li>Next session: <strong>The Engine Layer</strong> â€” what <code>LLMEngine</code> does inside <code>step()</code>, how <code>InputProcessor</code> tokenizes prompts, and how <code>EngineCoreClient</code> bridges to the core</li>
</ul>




<p><em>Generated from my <a href="https://github.com/ccwang/ai-study" rel="noopener noreferrer">ai-study</a> learning project.</em></p>

